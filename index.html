<!DOCTYPE html>
<html>
    <head>
  	<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
		<script src="http://www.mattmoocar.me/insight-DC/js/jquery.collapse.js"></script>
    
		<script type="text/javascript" src="http://www.mattmoocar.me/insight-DC/js/shCore.js"></script>
		<script type="text/javascript" src="http://www.mattmoocar.me/insight-DC/js/shBrushPython.js"></script>
		<script type="text/javascript" src="http://www.mattmoocar.me/insight-DC/js/shBrushSql.js"></script>
		<script type="text/javascript" src="http://www.mattmoocar.me/insight-DC/js/shBrushPlain.js"></script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
      	
    <title>Insight Workshop - Recommender Systems</title>
    <link rel="icon" type="image/png" href="/img/profile.png">
    
    <link href="http://www.mattmoocar.me/insight-DC/css/shCore.css" rel="stylesheet" type="text/css" />
    <link href="http://www.mattmoocar.me/insight-DC/css/shThemeDefault.css" rel="stylesheet" type="text/css" />
		<link href="http://www.mattmoocar.me/insight-DC/css/bootstrap.css" rel="stylesheet" type="text/css" />
    <link rel="stylesheet" href="http://www.mattmoocar.me/insight-DC/css/main.css">
		<link rel="stylesheet" href="http://www.mattmoocar.me/recsys/css/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" href="css/font-awesome/css/font-awesome.min.css">
    <link href="https://afeld.github.io/emoji-css/emoji.css" rel="stylesheet">

		<link rel="favicon-144-precomposed"  href="/img/profile.png">
		<link rel="shortcut icon" href="/img/profile.png">
    <link rel="stylesheet" media="screen" href="https://fontlibrary.org/face/junction" type="text/css"/> 

    <script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

    <script type="text/javascript" src="//use.typekit.net/sjt4iho.js"></script>
 		<script type="text/javascript">try{Typekit.load();}catch(e){}</script>
      
    <!--<script src="../node_modules/chart.js/dist/Chart.js"></script>-->
		<script src="http://www.mattmoocar.me/insight-DC/js/jquery-1.11.0.js"></script>

		
      	<style> 
        /***** BASE STYLES *****/ 
        body{
          background-image: url('figs/ignasi_pattern_s.png');
        }
        h1{
          font-size: 250%;
          font-family: 'JunctionRegular';
        }

        h2{
          font-size: 210%;
        }
        h3{
          font-size: 200%;
        }

        h4{
          font-size: 180%;
        }
        h5{
          font-size: 160%;
        }

        h6{
          font-size: 150%;
        }
        p {
           font-family: 'JunctionRegular';
           font-weight: normal;
           font-style: normal;
           font-size: 150%;
        } 
        code {
      		  color: inherit;
      		  background-color: rgba(0, 0, 0, 0.04);
      		}
		 pre:not([class]) {
		    background-color: white;
		  }
        blockquote {
          font-size: 21px;  
          margin: 90px 0 90px 0; 
          font-family: 'JunctionRegular';
        }

        cite {
          display: block;
          text-align: right;
          margin-top: 15px;
        }

        .firstcharacter { 
          float: left; 
          font-size: 108px; 
          line-height: 70px;  
          padding-right: 7px;  
        }  

        .fa {
          cursor: pointer;
        }  

        /** Navigation **/ 
          .main-navigation, .nav-toggle  {
            display: none;
          }

          #desktopNav	{
            position: absolute;
            height: 100%;
            background-color: #233140;
            top: 0;
            left: 0;
            z-index: 9999;
            width: 230px;
          }

          #desktopNav.fixedElement {
            position: fixed;
            height: 100%;
            top: 0;
            left: 0;
          } 


        nav ul li a {
          font-family: 'JunctionRegular';
          color: #E8E8E8;
          text-decoration: none;
          transition: color 0.3s;
          letter-spacing: 1px;
        } 

        nav ul li a:hover {
          color: #C7C7C7;
        } 

         .secondUL li {
            padding: 10px 10px 0 0;
          }

          .navHeadline {
            text-transform: uppercase;
            letter-spacing: 1px;
            font-weight: 600; 
            color: #4C91B8;
          }

        .navSection {
            margin-bottom: 20px;
          } 

          .nav-toggle {
            transform: translateY(-100%);
            overflow: hidden;
            height: 50px;
            background: rgba(29, 55, 69, 0.9);
            transition: transform 0.9s ease;
            color: #E9A857;
            font-size: 20px;
          } 

         #bootstrap-overrides  .nav-toggle.show {
            height: 50px;
            transform: translateY(0%);
            transition: transform 0.9s ease;
          }

         #bootstrap-overrides  .main-navigation {
            position: fixed;
            top: 0;
            left:0; 
            width: 270px;
            height: 100%;
            background: rgba(29, 55, 69, 0.9);
          }


        .center {
          max-width: 1170px;
          margin: 0 auto;
          position: relative;
        }
        .img-center{
          margin-left: 115px;
          margin-top: 40px;
          margin-bottom: 40px;
        }

        .wrapper {
          margin-left: 230px;
        } 	  
          
        .left {
          width: 50%;
          float:left;
        }

        .right {
          position: relative; 
          float:left;
          width: 50%;
        }  


        /** Exeptions **/
        article#one {
            margin-bottom: 50px;
          	padding: 0;
        }
          
        article#two {
            padding-top: 0
        } 
          
        article#four h2 {
            margin-top:.5em;
        } 
          
        article#eight .quote {
            margin-top: 30px 0 75px 0;
        } 
          
        article#eight .center .left, article#six .center .left {
            margin-right: 5%;
        } 
          
        article#eight .right, article#six .right {
            width: 45%;
        }
          
        article#eight .right img, article#six .right img {
            width: 100%;
        }  

        /*** Footer ***/
          footer {
            background-color: #1D3745; 
            width: 100%; 
            position: absolute; 
            z-index: 9999; 
            bottom: -70px;
          } 
          
           footer p {
            float: left;
          }

          .mktoForm {
            width: auto!important;
          }

          .bulkUpImage {
          	width: 46%;
          }	
          
          /***** Desktop Small *****/
          @media screen and (max-width: 1441px) {

            .center {
                max-width: 970px;
            }

            .nasties {
              top: 165px;
              width: 60%;
            }

            .container .sideText1, .container .sideText2, .container .sideText3, .container .sideText4, .container .sideText5 {
              margin-left: 60px;
              margin-right: auto;
            }

            .container .sideText1 p, 
.container .sideText2 p, .container .sideText3 p, .container .sideText4 
p, .container .sideText5 p {
              margin-left: 0;
            }

            .thickOrangeLine {
              display: none;
              top: 30px;
              width: 43px;
            }

            .bulkUpImage {
              width: 65%;
            }
          }  

          @media screen and (max-width: 1225px) {
            .center {
              max-width: 870px;
            }

            .bulkUpImage {
              width: 78%;
            }
          }

          @media screen and (max-width: 1170px) {
            .center {
              max-width: 770px;
            }

            footer {
              display: none;
            }

            .bulkUpImage { 
              width: 100%;
              margin-left: 10%;
            }
          }   

          @media screen and (max-width: 1024px) {

            .learnDesktop {
              right: -155px
            }

            .center {
              max-width: 670px;
            }
          }  

         /* Style the list */
ul.tab {
    list-style-type: none;
    margin: 0;
    padding: 0;
    overflow: hidden;
    border: 1px solid #ccc;
    background-color: #f1f1f1;
}

/* Float the list items side by side */
ul.tab li {float: left;}

/* Style the links inside the list items */
ul.tab li a {
    display: inline-block;
    color: black;
    text-align: center;
    padding: 14px 16px;
    text-decoration: none;
    transition: 0.3s;
    font-size: 17px;
}

/* Change background color of links on hover */
ul.tab li a:hover {background-color: transparent;}

/* Create an active/current tablink class */
ul.tab a:focus {background-color: #ddd;}

/* Style the tab content */
.tabcontent {
    display: none;
    padding: 6px 12px;
    border: 1px solid #ccc;
    border-top: none;
}

          
        </style>


<style>.mktoGen.mktoImg {display:inline-block; line-height:0;}</style>
  	</head>
  	<body id="bootstrap-overrides">
      <style type="text/css"> 
      #bootstrap-overrides hr.star-light, hr.star-prinary{  
        border-color: transparent;
        margin-left: auto;
          margin-right: auto;
        }
      
      #bootstrap-overrides hr.star-light {  
        border-color: #18BC9C;
        margin-left: auto;
          margin-right: auto;
        }
       #bootstrap-overrides  hr.star-light:after {
      background-color: transparent;
      color: #2C3E50;}

      #bootstrap-overrides  hr.star-primary:after {
      background-color: transparent;
      color: #2C3E50;}

      #bootstrap-overrides div.post-overlay-header{
        background-color: #2C3E50;
        display:inline-block;
      margin-left:30px;
      margin-right:auto;
      width:400px;
      height:312px;
      }
      canvas {
          padding-left: 0;
          padding-right: 0;
          margin-left: auto;
          margin-right: auto;
          display: block;
          width: 90px;
      }
      img{
        border: 3px solid #2C3E50;
        padding: 5px;
      }
      </style>
  	</body>
	<body class="layout" id="bodyId">
      	
		<div class="extraWrapper" style="position:relative;">	
			<!-- DESKTOP FIXED NAV -->
         	<div id="desktopNav">
            <div style="position: relative; height: 100%;">
               <nav>
                  <ul class="firstUL">	
                    <br>
                    <li class="navSection"><span class="navHeadline"><a style="color: #4C91B8;" href="http://www.mattmoocar.me">Home</a></span>
                      <ul class="secondUL"> 
                          <li><a href="http://www.mattmoocar.me">Matt Moocarme</a></li>
                          <li><a href="http://www.mattmoocar.me/blog">Blog</a></li>
                      </ul>
                    </li>
                    <li class="navSection"><span class="navHeadline">Contents</span>
                      <ul class="secondUL">	
                          <li><a href="/examples/SmoothPageScroll/#one">Definition</a></li>
                          <li><a href="/examples/SmoothPageScroll/#two">Content vs Collaborative</a></li>
                          <li><a href="/examples/SmoothPageScroll/#three">Explicit vs Implicit</a></li>
                          <li><a href="/examples/SmoothPageScroll/#four">Algorithms</a></li>
                          <li><a href="/examples/SmoothPageScroll/#five">Validation</a></li>
                          <li><a href="/examples/SmoothPageScroll/#six">Best Practices</a></li>
                          <!--<li><a href="/examples/SmoothPageScroll/#seven">Emailing Results and Scheduling the Model</a></li>-->
                          <li><a href="/examples/SmoothPageScroll/#eight">Conclusion</a></li>
                          <!--<li><a href="/examples/SmoothPageScroll/#forecast">Related Projects</a></li>-->
                      </ul>
                    </li>

                  </ul> 
              </nav> 
            </div>
          	</div>
	<div class="wrapper" id="header">
<p><br></p>
<h1 align="center" style = "margin: 0;"><big>Recommender Systems</big></h1>
<header>
    <hr class="star-light" style='background-color:transparent; color: transparent;'>
</header>
</div>
<div class="wrapper">
				<!-- Resolution 1 -->
				<article id="one">
      				<div class="center">
              			<div>
              				<h2>Definition</h2>
                      <p>Recommender systems are a subclass of information filtering system that seek to predict the "rating" or "preference" that a user would give to an item. </p>
                      <p>For the demonstrative purposes of this post I will be using a dataset of beer reviews from a popular beer rating website. The full python script to accompany this work can be found <a target='_blank' href="python-code/">here</a>, and a link to the dataset can be found <a href="http://s3.amazonaws.com/beer-reco/data/beer_reviews.csv" target = "_blank">here</a> (170Mb).</p>
                      <br>
                      <p>Resources</p>
                      <li><p><a href="http://bugra.github.io/work/notes/2014-04-19/alternating-least-squares-method-for-collaborative-filtering/">Blog post on alternating least squares</a></p></li>
                      <li><p><a href="https://jeremykun.com/2016/04/18/singular-value-decomposition-part-1-perspectives-on-linear-algebra/">Blog post on singular value decomposition by Jeremy Kun</a></p></li>
                      <li><p><a href="https://making.dia.co/embedding-everything-for-anything2anything-recommendations-fca7f58f53ff#.pms8hvlyj">Blog post on matrix factorization by Ethan RosenThal of Dia & Co</a></p></li>
                      <li><p>Jupyter notebook on validating recommendation algoritms on Insight dropbox</p></li>
              			</div>
              			<div class="left"></div>
                    <div>
                      <h2>Na&iuml;ve Solution</h2>
                      <p>Recommend the most popular item. This may be popularity in terms of total number of plays of songs, total number of purchases, or average 5 star ratings.</p>
                    </div>
      				</div>  
    			</article>  
    			<!-- End Resolution 1 -->
    
				<!-- Resolution 2 -->
				<article id="two">  
          			
         	 			<div class="center">  
                <p><br></p>  
            				<h2>Content vs Collaborative</h2>  
                    <p><br></p>  
                    <h4>Content</h4>
                    <p>Recommending items based on similarities between their features</p>
                    <p>For in the case of this recommending beer the features may be the proprtion of ingredients, colour, clarity, alcohol content, bitterness, etc. One beer is similar to another if it has many of the same features in common.
                    <p><b>Pro: </b>It can be useful in the interest of the "cold-start" problem, in which you may not have any user feedback.</p>
                    <p><b>Con: </b>Doesn't take user information into account.</p>
                    </p>
                    <br>
                    <h4>Collaborative</h4>
                    <p> Collaborative filtering approaches building a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in.
                    </p>
                    <p><b>Pro: </b>No need to know about item content.</p>
                    <p><b>Pro: </b>"Item cold-start" problem is avoided.</p>
                    <p><b>Pro: </b>User interest taken into consideration.</p>
                    <p><b>Pro: </b>Can capture subtle relationships.</p>
                    <p><b>Con: </b>Usually deal with large, sparse matrices.</p>
                    <p><b>Con: </b>Grey sheep whose taste and preference do not consistently agree with other users' preference are to to recommend for.</p>
                    <p><b>Con: </b>Grey sheep whose taste and preference do not consistently agree with other users' preference are to to recommend for.</p>

                    <p><br></p>
            				<div class="left">
              					<div class="container">
                					<div class="sideText2">
                  						<span class="line"></span>
                  						<p></p>
                					</div>
              					</div>
              					
            				</div>   
         				</div>
         			  
      			</article>
      			<!-- End Resolution 2 -->
   
	  			<!-- Resolution 3 -->
	  			<article id="three">
          			<div class="center">
          				<p><br></p>

        				<h2>Explicit vs Implicit</h2>
              				<h4>Explicit</h4>
                      <p><br></p>
                      <p>Explicit user preference requires user input, such as the ratings of movies on netflix.</p>
                      <p><br></p>
                      <h4>Implicit</h4>
                      <p><br></p>
                      <p>Another, more subtle way to passively guage a users preference for an item, is to look implicit factor such as purchase history, watching habits and browsing activity. For example, if a person has purchased a large amount of beer, or listened to a song a large number of times, we can implicitly infer that the user likes this particular beer or song, without explicitly asking them for their rating on the item.</p>
                      
                      <br>
                      <!--
                      <div data-collapse>
                        <h4> Check Out first 30 rows &#9660;</h4>
                          <div style="overflow:auto">
      
                          </div>
                      </div>
                      -->
                      
                      <h3></h3>
                      <p></p>
                      <br>
                <pre class= "brush: python">beer_data = pd.read_csv('beer_reviews/beer_reviews.csv')

test_data = beer_data.groupby('review_profilename', as_index=False).apply(lambda x: x.loc[np.random.choice(x.index, 1, replace=False),:])
l1 = [x[1] for x in test_data.index.tolist()]

train_data = beer_data.drop(beer_data.index[l1]).dropna()

train_data['review_profilename'] = train_data['review_profilename'].astype("category")
train_data['beer_name'] = train_data['beer_name'].astype("category")

print "Unique users: %s" % (len(train_data['review_profilename'].unique()))
print "Unique beers: %s" % (len(train_data['beer_name'].unique()))

# create a sparse matrix of all the artist/user/play triples
reviews = csc_matrix((train_data['review_overall'].astype(float), 
                   (train_data['beer_name'].cat.codes, 
                    train_data['review_profilename'].cat.codes)))      
                             </pre>
                             <p><br></p>
                      <!--<div data-collapse>-->
</div>
            </article>
          <!-- End Resolution 3 -->
      
          <!-- Resolution 4 -->
          <article id="four">
            <div class = 'center' style="position: relative;">
                <p><br></p>
<h2>Algorithms</h2>     
<br>
<p>Most methods use cosine similarity to determine if a user or item is similar to others.</p>
<div style="text-align: center;">
<p><span class="math"> \( Similarity = \frac{AA^T}{\|A\|^2}\) </span></p>  
</div>
<br>
<h4>User-User and Item-Item</h4>
<p><br></p>
<div class = 'row'>
<div class = 'col-sm-6'>
  <p style="font-size:150%">User-User</p>
  <br>
  <img src="figs/userSim.png" width="100%">
</div>
<div class = 'col-sm-6'>
  <p style="font-size:150%">Item-Item</p>
  <br>
  <img src="figs/songSim.png" width="100%">
</div>
</div>
<p><br></p>
<p>In general item-item performs better than user-user. So for the remainder we assume item-item unless stated otherwise.</p>
<br>
<h2>Matrix Factorization</h2>
<br>
<p>Matrix factorization is an important component of collaborative-filtering based approaches of recommendation systems as it allows us to approximate the original matrix. While this can be compuationally quite demanding it generally pays off, since whenever any matrix multiplication needs to be performed, it can be done much faster due to the reduced size of the matrix after factorization.</p>
<p>It is important to note that matrix factorization is not the end result, but a tool to help reduce the size of the matrix for further computations.</p>
<br>
<h4>Singular Value Decomposition</h4>
<br>       
<p>In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix.</p>
<p>Matrix factorization is an important component of collaborative-filtering based approaches of recommendation systems as it allows us to approximate the original matrix. </p>
<div style="text-align: center;">
<p><span class="math"> \( D=W\Sigma T^T\) </span></p>  
</div>
<br>
<center><img src="figs/mf3.jpg" width = "80%"></center>
<p><br></p>
<p>We say that our original sparse matrix is similar to a linear map, and thus can make algorithms that predict how people rate beers with surprisingly good accuracy. So if you know that a beer such as Blue Moon gets ratings low reviews from a certain group of users, then a new person may review Blue Moon based on a linear combination of how well they align with that group of people, if they strongly align, the model will predict a low rating for Blue Moon. By “align" I mean the user has similar ratings across other beers to the users in the group. In other words, a linear combination of users epitomize the process of rating beers.</p>
<br>
<p>
<b>SVD provides an useful approach to matrix factorization that represents the process of people rating beers.</b> By changing the basis of one or both vector spaces involved, we isolate the different (orthogonal) characteristics of the process. In the context of our movie example, “factorization” means the following:</p>
    
    <li><p>Come up with a basis set of vectors v<sub>1</sub>, v<sub>2</sub>, ..., v<sub>i</sub> so that every beer can be written as a linear combination of the v<sub>i</sub>.</p></li>
    <li><p>Do the analogous thing for people to get a basis set for beer drinkers.</p></li>
    <li><p>Do steps 1 and 2 in such a way that the map <span class="math"> \(\Sigma \) </span> is diagonal with respect to both new bases simultaneously.</p></li>
<p>
One might think of the v<sub>i</sub> as “idealized beers" and the p<sub>j</sub> as “idealized beer drinkers”. For example, an “idealized beer" may be a stout, and its corresponding “idealized beer drinker" may be someone that only drinks stouts, though its unlikely the "idealized beer" will fall into categories corresponding to types of beers.</p>
<p><br></p>
<h4>Alternating Least Squares</h4>
<p><br></p>
<p>The algorithmn of alternating least square aims to factorize the original matrix and obtain two matrices similar to <span class="math"> \(W \) </span> and <span class="math"> \(T \) </span> that were obtained using SVD. To do this we first estimate <span class="math"> \(W \) </span> using a random initialization of <span class="math"> \(T \) </span> and estimate <span class="math"> \(T \) </span> by using <span class="math"> \(W \) </span>. After enough number of iterations, we should hopefully reach convergence  where either the matrices <span class="math"> \(W \) </span> and <span class="math"> \(T \) </span> are no longer changing or the change is below some tolerance.</p>
<p><br></p>
<pre class = "brush: python">def alternating_least_squares(Cui, factors, regularization=0.01,
                              iterations=15, use_native=True, num_threads=0,
                              dtype=np.float64):
    """ factorizes the matrix Cui using an implicit alternating least squares
    algorithm
    Args:
        Cui (csr_matrix): Confidence Matrix
        factors (int): Number of factors to extract
        regularization (double): Regularization parameter to use
        iterations (int): Number of alternating least squares iterations to
        run
        num_threads (int): Number of threads to run least squares iterations.
        0 means to use all CPU cores.
    Returns:
        tuple: A tuple of (row, col) factors
    """
    #_check_open_blas()

    users, items = Cui.shape

    X = np.random.rand(users, factors).astype(dtype) * 0.01
    Y = np.random.rand(items, factors).astype(dtype) * 0.01

    Cui, Ciu = Cui.tocsr(), Cui.T.tocsr()

    solver = least_squares

    for iteration in range(iterations):
        s = time.time()
        solver(Cui, X, Y, regularization, num_threads)
        solver(Ciu, Y, X, regularization, num_threads)
        print "finished iteration %i in %s" % (iteration, time.time() - s)

    return X, Y


def least_squares(Cui, X, Y, regularization, num_threads):
    """ For each user in Cui, calculate factors Xu for them
    using least squares on Y.
    Note: this is at least 10 times slower than the cython version included
    here.
    """
    users, factors = X.shape
    YtY = Y.T.dot(Y)

    for u in range(users):
        # accumulate YtCuY + regularization*I in A
        A = YtY + regularization * np.eye(factors)

        # accumulate YtCuPu in b
        b = np.zeros(factors)

        for i, confidence in nonzeros(Cui, u):
            factor = Y[i]
            A += (confidence - 1) * np.outer(factor, factor)
            b += confidence * factor

        # Xu = (YtCuY + regularization * I)^-1 (YtCuPu)
        X[u] = np.linalg.solve(A, b)

factors = 50
als_user_factors, als_beer_factors = alternating_least_squares(bm25_weight(train_data), factors)
als_ii = TopRelated_itemitem(als_beer_factors.T)
</pre>
<p><br></p>
<h4>Implicit Matrix Factorization</h4>
<br>
<p>Defined by:</p>
<ul>
  <li><p>No negative feedback - entries must be positive.</p></li>
  <li><p>Inherently noisy - For example, someone may drink a lot of PBR, because it is cheap, rather than because they like it.</p></li>
  <li><p>While numerical value of explicit feedback indicates <i>preference</i>, the numerical value of implicit
feedback indicates <i>confidence</i> that a user prefers an item.</p></li>
</ul>
<p>Code can be found from Chris Johnson's blog <a href="https://github.com/MrChrisJohnson/implicit-mf">here</a>, based on <a href="http://yifanhu.net/PUB/cf.pdf">this paper</a>, and demonstrated below:</p>
<br>
<pre class="brush: python">class ImplicitMF():

    def __init__(self, counts, num_factors=40, num_iterations=30,
                 reg_param=0.8):
        self.counts = counts
        self.num_users = counts.shape[0]
        self.num_items = counts.shape[1]
        self.num_factors = num_factors
        self.num_iterations = num_iterations
        self.reg_param = reg_param

    def train_model(self):
        self.user_vectors = np.random.normal(size=(self.num_users,
                                                   self.num_factors))
        self.item_vectors = np.random.normal(size=(self.num_items,
                                                   self.num_factors))

        for i in xrange(self.num_iterations):
            t0 = time.time()
            print 'Solving for user vectors...'
            self.user_vectors = self.iteration(True, csr_matrix(self.item_vectors))
            print 'Solving for item vectors...'
            self.item_vectors = self.iteration(False, csr_matrix(self.user_vectors))
            t1 = time.time()
            print 'iteration %i finished in %f seconds' % (i + 1, t1 - t0)

    def iteration(self, user, fixed_vecs):
        num_solve = self.num_users if user else self.num_items
        num_fixed = fixed_vecs.shape[0]
        YTY = fixed_vecs.T.dot(fixed_vecs)
        eye1 = eye(num_fixed)
        lambda_eye = self.reg_param * eye(self.num_factors)
        solve_vecs = np.zeros((num_solve, self.num_factors))

        t = time.time()
        for i in xrange(num_solve):
            if user:
                counts_i = self.counts[i].toarray()
            else:
                counts_i = self.counts[:, i].T.toarray()
            CuI = diags(counts_i, [0])
            pu = counts_i.copy()
            pu[np.where(pu != 0)] = 1.0
            YTCuIY = fixed_vecs.T.dot(CuI).dot(fixed_vecs)
            YTCupu = fixed_vecs.T.dot(CuI + eye1).dot(csr_matrix(pu).T)
            xu = spsolve(YTY + YTCuIY + lambda_eye, YTCupu)
            solve_vecs[i] = xu
            if i % 1000 == 0:
                print 'Solved %i vecs in %d seconds' % (i, time.time() - t)
                t = time.time()

        return solve_vecs


</pre>
<p><br></p>
<h4>Other Methods</h4>
<br>
<p>Incorporating other user and item features with collaborative filtering are known as hybrid recommender systems. An example of how to incorporate features into your recommendation has been handled by the folks at <a href="https://www.lyst.com/">Lyst</a> with their python package <a href="https://github.com/lyst/lightfm">LightFM</a>.</p><br>
<pre class="brush: python">from lightfm import LightFM
model = LightFM(loss='warp')
model.fit(train_data, epochs=30, num_threads=2, user_features=None, item_features=None)

</pre>
<div class="left">     	
	<div class="container">
		<div class="sideText3">
				<span class="line"></span>
				
		</div>
	</div>	   

</div>
      					
<p><br></p>
				</article>
  				<!-- End Resolution 4 --> 
  
  				<!-- Resolution 5 --> 
    			<article id="five">
      				<div class="center">      
      					<p><br></p>
          				<h2>Validation</h2>
    <h4>M-fold validation</h4>
    <br>
    <p>A common strategy for splitting recommendation data into training and test sets is leave-k-out. Here, a split percentage is chosen (e.g., 80% train, 20% test) and the test percentage is selected randomly from the user-item pairs with non-zero entries.</p>
    <br>
    <h4>Miss-k-out</h4>
    <br>
    <p>A useful variant that combines leave-k-out and standard K-fold crossvValidation is (sometimes) called M-fold cross validation. The idea is simple - split the data into a training set and holdout set. On the holdout set, perform leave-k-out.</p>
    <br>
    <h2>Evaluation Metrics</h2>
    <h4>Mean Average Precision</h4>
    <br>
    <p>This is simply the mean across all users of the average precision at k. The average precision at k (AP@k) is defined as:</p>
    <div style="text-align: center;">
      <p><span class="math"> \( AP@k = \sum_{n=1}^k\frac{precision@n}{min(n, y_{test})}\) </span></p>  
    </div>
    <p><br></p>
    <p>For example if the evaluation metric were MAP@100, the recommendation system chooses 100 beers, and its trying to predict a number of beers, <span class="math"> \(n \) </span> that have been left out for validation. If the missing beers are correctly choosen and ranked highly in the list of 100, the MAP will be close to 1. If they were not in the list whatsoever, the MAP would be 0. </p>
    <br>
    <pre class="brush: python">def apk(actual, predicted, k=10):
    """
    Computes the average precision at k.
    This function computes the average prescision at k between two lists of
    items.
    Parameters
    ----------
    actual : list
             A list of elements that are to be predicted (order doesn't matter)
    predicted : list
                A list of predicted elements (order does matter)
    k : int, optional
        The maximum number of predicted elements
    Returns
    -------
    score : double
            The average precision at k over the input lists
    """
    if len(predicted)>k:
        predicted = predicted[:k]

    score = 0.0
    num_hits = 0.0

    for i,p in enumerate(predicted):
        if p in actual and p not in predicted[:i]:
            num_hits += 1.0
            score += num_hits / (i+1.0)

    if not actual:
        return 0.0

    return score / min(len(actual), k)

def mapk(actual, predicted, k=10):
    """
    Computes the mean average precision at k.
    This function computes the mean average prescision at k between two lists
    of lists of items.
    Parameters
    ----------
    actual : list
             A list of lists of elements that are to be predicted 
             (order doesn't matter in the lists)
    predicted : list
                A list of lists of predicted elements
                (order matters in the lists)
    k : int, optional
        The maximum number of predicted elements
    Returns
    -------
    score : double
            The mean average precision at k over the input lists
    """
    return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])
    </pre>
    <p><br></p>
    <h4>Normalized Discounted Cumulative Gain</h4>
    <br>
    <p>The normalized discounted cumulative gain (NDCG) is similar to the MAP, however strongly emphasizes that items with high relevance should be placed early in the ranked list, and that order is important.</p>
    <div style="text-align: center;">
      <p><span class="math"> \( DCG@k = \sum_{i=1}^k\frac{2^{rel_i}-1}{log_2(i+1)}\) </span></p>  
    </div>
    <p><br></p>
    <p>The NDCG@k is the normalised result of DCG@k so we can obain a value beween 0 and 1.  </p>
    <br>
    <pre class="brush: python">def dcg_at_k(r, k, method=0):
    """Score is discounted cumulative gain (dcg)
    Relevance is positive real values.  Can use binary
    as the previous methods.
    Code source: https://gist.github.com/bwhite/3726239
    Example from
    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf
    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]
    >>> dcg_at_k(r, 1)
    3.0
    >>> dcg_at_k(r, 1, method=1)
    3.0
    >>> dcg_at_k(r, 2)
    5.0
    >>> dcg_at_k(r, 2, method=1)
    4.2618595071429155
    >>> dcg_at_k(r, 10)
    9.6051177391888114
    >>> dcg_at_k(r, 11)
    9.6051177391888114
    Args:
        r: Relevance scores (list or numpy) in rank order
            (first element is the first item)
        k: Number of results to consider
        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]
                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]
    Returns:
        Discounted cumulative gain
    """
    r = np.asfarray(r)[:k]
    if r.size:
        if method == 0:
            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))
        elif method == 1:
            return np.sum(r / np.log2(np.arange(2, r.size + 2)))
        else:
            raise ValueError('method must be 0 or 1.')
    return 0.


def ndcg_at_k(r, k, method=0):
    """Score is normalized discounted cumulative gain (ndcg)
    Relevance is positive real values.  Can use binary
    as the previous methods.
    Code source: https://gist.github.com/bwhite/3726239
    Example from
    http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf
    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]
    >>> ndcg_at_k(r, 1)
    1.0
    >>> r = [2, 1, 2, 0]
    >>> ndcg_at_k(r, 4)
    0.9203032077642922
    >>> ndcg_at_k(r, 4, method=1)
    0.96519546960144276
    >>> ndcg_at_k([0], 1)
    0.0
    >>> ndcg_at_k([1], 2)
    1.0
    Args:
        r: Relevance scores (list or numpy) in rank order
            (first element is the first item)
        k: Number of results to consider
        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]
                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]
    Returns:
        Normalized discounted cumulative gain
    """
    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)
    if not dcg_max:
        return 0.
    return dcg_at_k(r, k, method) / dcg_max
  </pre>
</div>
    			</article>
  				<!-- End Resolution 5 --> 

				<!-- Resolution 6 --> 
    			<article id="six">
      				<div class="center">  
						    <p><br></p>
                  <h2>Best Practices</h2>
                  <br>
                  <p>Recommendation problems can be fun, but tough to evaluate. It's important to absorb the points above, and think about what approaches make the most sense for your situation.</p>
                  <p>Giving advice that will work generically is hard, but the following list will probably cover best practices is most situations:</p>
                    <ul>
                    <li><p>Think carefully about the right strategy for splitting your data. Leave-k-out is simple, but may not be well-suited to data which has many users with few entries. M-fold is a good representation of your final model, but may require enough data and available computation.</p></li>
                    <li><p>Metrics should be chosen with the end goal in mind. Error and classification metrics are good for assessing overall performance, but if recommending N items, one should at least consider rank-based metrics.</p></li>
                    <li><p>Know the details of metrics, be able to provide a plain language explaination and justify your choices.</p></li>
                    <li><p>When practically feasible, check that your metrics are statistically significant when comparing different models. You may need to run more train/test experiments.</p></li>
                    <li><p>It's always a good idea to examine how metrics vary with subgroups of the data. For example, how does a given metric change for users with few item entries versus those with lots.</p></li>
                    <li><p>Convince your audience - explain the metrics, but help them connect. Qualitative approaches can help here.</p></li>
</ul>
          				<div class="left">
            				<p></p> 
            				<div class="container">
               					<div class="sideText5">
                  					<span class="line"></span>
                	 				<p></p>
                				</div>
            				</div>	
      						<p></p>
        				</div>
          				<div class="right">
        				</div> 
       				</div>       
    			</article>
    			<!-- End Resolution 6 --> 

				<!-- Resolution 7 -->  
    			<article id="seven" style="/*background: rgba(247, 247, 247, 0.6);;*/ padding-bottom: 0;">	 
					<div class="center">
					   <div style="clear:both;"></div>	
      				</div>   	
    			</article>
  				<!-- End Resolution 7 --> 
  	
				<!-- Resolution 8 -->  
    			<article id="eight">
      				<div class="center">	
              			<h2>Let's Drink - Choose Your Beer <i class="fa fa-beer" aria-hidden="true"></i>
</h2>
                    <br>
                    <div><center><iframe src="http://s3.amazonaws.com/beer-reco/venn_test.html" width = "100%" height = "550" scrolling="no" style="border: 1px black solid; "></iframe></center></div>
              			<div class="left"> 	
                			<div class="container">
                				<div class="sideText4">
                  					<span class="line"></span>
                		  			<p></p>
                				</div>
              				</div>	  
      						<p></p>
            			</div>
              			<div class="right">
              			</div>  
              			<div style="clear: both;"></div>
                     </div> 
              		 <div class="center">
              		</div>  	
    			</article>
  				<!-- End Resolution 8 --> 
	
    			<!-- Resolution 9-->      
    			<article id="forecast" style=" display: block; margin-bottom: 0px; ">
      					<div class="center"> 
              				
      					</div>
      				
    			</article>
    			<!-- End Resolution 9 --> 
              
        	</div> <!--extra wrapper-->
         	<footer class="text-center">
        <!--<div class="footer-below">
            <div class="container">
                <div class="row">
                    <div class="col-lg-12">
                        <h4>Matthew Moocarme - moocarme@gmail.com</h4>
                    </div>
                </div>
            </div>
        </div>
    </footer>-->
    	</div><!-- end wrapper -->
 	<script type="text/javascript" src="//munchkin.marketo.net//munchkin-beta.js"></script><script>Munchkin.init('062-HAC-076', {customName: 'past-forward-2016', wsInfo: 'j1RR'});</script>
 	</body>


<style>
#mktoForm_375 {
    width: auto!important;
}
</style>
<script type="text/javascript">
     SyntaxHighlighter.all()
</script>



<script>
// Get the element with id="defaultOpen" and click on it


$(document).ready(function() {
  
    
  
	var sections = $('article'), 
        nav = $('nav'), 
        nav_height = nav.outerHeight();
 
	$(window).on('scroll', function () {

  		var cur_pos = $(this).scrollTop();
 
  		sections.each(function() {
    	var top = $(this).offset().top - nav_height,
        bottom = top + $(this).outerHeight();
 
    	if (cur_pos >= top && cur_pos <= bottom) {
      		nav.find('a').removeClass('active');
      		sections.removeClass('active');
 
      		$(this).addClass('active');
      		nav.find('a[href="/examples/SmoothPageScroll/#'+$(this).attr('id')+'"]').addClass('active');
    	}
  		});
      
      	if ($('#one .left').isOnScreen()) {
    		var wScroll = $(this).scrollTop();
          	var $sideText1 = $('.sideText1');
            var containerScroll = wScroll - $sideText1.parent().offset().top + 1000;
      		$sideText1.css({
       			'transform' : 'translate(0px, +'+ containerScroll/10 +'%)' 
      		});  
      	}
      	
      	if ($('#two .left').isOnScreen()) {
    		var wScroll = $(this).scrollTop();
          	var $sideText2 = $('.sideText2');
          	var containerScroll2 = wScroll - $sideText2.parent().offset().top + 1000;
      		$sideText2.css({
       			'transform' : 'translate(0px, +'+ containerScroll2/10 +'%)' 
      		});  
      	}
      	
      	 if ($('#four .left').isOnScreen()) {
    		var wScroll = $(this).scrollTop();
          	var $sideText3 = $('.sideText3');
          	var containerScroll3 = wScroll - $sideText3.parent().offset().top + 1000;
      		$sideText3.css({
       			'transform' : 'translate(0px, +'+ containerScroll3/10 +'%)' 
      		});  
      	}
      
      	if ($('#eight .left').isOnScreen()) {
    		var wScroll = $(this).scrollTop();
          	var $sideText4 = $('.sideText4');
          	var containerScroll4 = wScroll - $sideText4.parent().offset().top + 1000;
      		$sideText4.css({
       			'transform' : 'translate(0px, +'+ containerScroll4/10 +'%)' 
      		});  
      	}
      
      	if ($('#six .left').isOnScreen()) {
    		var wScroll = $(this).scrollTop();
          	var $sideText5 = $('.sideText5');
          	var containerScroll5 = wScroll - $sideText5.parent().offset().top + 1000;
      		$sideText5.css({
       			'transform' : 'translate(0px, +'+ containerScroll5/10 +'%)' 
      		});  
      	}
      
      	/** Measures the height of hero then nav bar appears after it **/
      	var heroHeight = $('.hero').height();
      
      	if ($(this).scrollTop() > heroHeight) {
          $('.nav-toggle').addClass('show');
        } else {
           $('.nav-toggle').removeClass('show');
        }

      	/** For Desktop Nav only (from 1170px wide) **/
      	if ($(this).scrollTop() > heroHeight){ 
    		$('#desktopNav').addClass('fixedElement'); 
  		} else {
    		$('#desktopNav').removeClass('fixedElement'); 
  		}	
	});  
  
  	/**** Smooth Scrolling ****/
	$('a[href*=#]:not([href=#])').click(function() {
    	if (location.pathname.replace(/^\//,'') == this.pathname.replace(/^\//,'') 
        	|| location.hostname == this.hostname) {

        	var target = $(this.hash);
        	target = target.length ? target : $('[name=' + this.hash.slice(1) +']');
           	if (target.length) {
            	$('html,body').animate({
                	scrollTop: target.offset().top
            	}, 1000);
            	return false;
        	}
    	}
	});
  	
  	
}); 

  
  /*** Navigation menu toggle ***/
  $('.nav-toggle').on('click', function(){
    $('.wrapper').toggleClass('open');
    $('.main-navigation-left').toggleClass('open');
    $('.nav-toggle').toggleClass('open');
  });
  
  /*Desktop Fixed Nav*/
  var stickyTop = $('#desktopNav').offset().top;
  
  
  /** Function to see when a div is in view **/
  $.fn.isOnScreen = function(){
    var win = $(window);
    var viewport = {
        top : win.scrollTop(),
        left : win.scrollLeft()
    };
    
    viewport.right = viewport.left + win.width();
    viewport.bottom = viewport.top + win.height();
    
    var bounds = this.offset();
    bounds.right = bounds.left + this.outerWidth();
    bounds.bottom = bounds.top + this.outerHeight();
    
    return (!(viewport.right < bounds.left || viewport.left > bounds.right || viewport.bottom < bounds.top || viewport.top > bounds.bottom));
  };
</script></html>
